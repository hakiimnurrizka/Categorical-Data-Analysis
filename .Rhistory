###Contingency Table Analysis###
#Probably the most popular way to represent categorical data. Contingency table simplifies categorical data
#type into table which cells are the count for each combination of category.
#We'll illustrate how to create, manipulate, and use the contingency table to gain insight from
#a categorical data type.
library(ISLR) #We use the wage data from this library
library(tidyverse)
library(Rfast)
library(MASS)
install.packages("ISLR")
###Contingency Table Analysis###
#Probably the most popular way to represent categorical data. Contingency table simplifies categorical data
#type into table which cells are the count for each combination of category.
#We'll illustrate how to create, manipulate, and use the contingency table to gain insight from
#a categorical data type.
library(ISLR) #We use the wage data from this library
data("Wage")
View(Wage)
#Create a new category based on the wage
Wage$wage_cat = as.factor(ifelse(Wage$wage>median(Wage$wage),"Above","Below"))
#Examine wage category against some other variables
table(Wage$jobclass,Wage$wage_cat)
table(Wage$age,Wage$wage_cat
table(Wage$age,Wage$wage_cat
)
table(Wage$age,Wage$wage_cat)
table(Wage$region,Wage$wage_cat)
table(Wage$education,Wage$wage_cat)
table(Wage$maritl,Wage$wage_cat
table(Wage$maritl,Wage$wage_cat)
table(Wage$maritl,Wage$wage_cat)
table(Wage$race,Wage$wage_cat
table(Wage$race,Wage$wage_cat)
table(Wage$race,Wage$wage_cat)
#Examine wage category against some other variables
table(Wage$jobclass,Wage$wage_cat)
table(Wage$education,Wage$wage_cat)
table(Wage$race,Wage$wage_cat)
#We'll focus on the wage category vs  job class
con1 = table(Wage$jobclass,Wage$wage_cat)
#To further see the difference between the job class and wage category, we can do a mosaicplot that represent
#the density for each cells relative to total number of observation
mosaicplot(con1)
#Next we can see the proportion of each cell relative to overall number of observation
prop.table(con1)
prop.table(con1, margin = 1)
prop.table(con1, margin = 2)
#Next we can do whats known as chi-square test of independence.
#This is a hypothesis test to inspect whether job class is independent with the wage category or not
chisq.test(con1)
#From the chi square test above, it can be concluded that job class is not independent to the wage category.
#This is a strong indication of the relationship between the two categories.
#There is also an alternative the chi square test called the fisher exact test, this test is used instead of the
#chi square test when the number of observation is relatively low.
fisher.test(con1)
#Next we'll try log linear modelling for the data
loglm( ~ 1 + 2, data = con1)
#Next we'll try log linear modelling for the data
loglm( ~ 1 + 2 + 1:2, data = con1)
#Next we'll try log linear modelling for the data
#First we convert the data into a frequency table
as.data.frame(as.table(con1))
#Next we'll try log linear modelling for the data
#First we convert the data into a frequency table
freq1 = as.data.frame(as.table(con1))
colnames(freq1) = c("Job_class", "Wage_median", "Freq")
#Then fitting log linear model using glm
mod0 = glm(Freq ~ Wage_median+Job_class+Wage_median:Job_class, data = freq1, family = poisson)
summary(mod0)
freq1
#Then weinspect the overall utility of the log linear model using deviance statistics
pchisq(deviance(mod0), df = df.residual(mod0), lower.tail = F)#compute p value for the deviance statistics
#The p value is very low even R function from above gives output of 0
#Null hypothesis that the expected frequency satisfy the given model is not accepted. In other words, the model
#fit seems to be poor. But at the same time, by using rule of thumb on the residual deviance against the degree
#of freedom, the model appears to be a good fit for the data.
#Therefore, we'll compapre side by side between the observed frequency and fitted frequency based on this model
cbind(mod0$data, fitted(mod0))
#Then weinspect the overall utility of the log linear model using deviance statistics
deviance(mod0)
df.residual(mod0)
#This confirm the problem of "exact fit" which implies that the proposed model is exactly the same as the data.
#We particularly not against such model, but we also don't get any useful inference from this type of model.
#As such, we propose a new loglinear model as follow and repeat exactly the same procedure as above
mod1 = glm(Freq ~ 0+Wage_median+Job_class+Wage_median:Job_class, data = freq1, family = poisson)
summary(mod1)
#This confirm the problem of "exact fit" which implies that the proposed model is exactly the same as the data.
#We particularly not against such model, but we also don't get any useful inference from this type of model.
#As such, we propose a new loglinear model as follow and repeat exactly the same procedure as above
mod1 = glm(Freq ~ Wage_median+Job_class, data = freq1, family = poisson)
summary(mod1)
pchisq(deviance(mod1), df = df.residual(mod1), lower.tail = F)
cbind(mod1$data, fitted(mod1))
#The second proposed model has poor fit to the data
mod2 = glm(Freq ~ Job_class, data = freq1, family = poisson)
summary(mod2)
mod3 = glm(Freq ~ Job_class, data = freq1, family = poisson)
mod3 = glm(Freq ~ Wage_median, data = freq1, family = poisson)
summary(mod3)
#Log likelihood test can be applied to test independence between 2 categorical variables
loglm(con1)
#Log likelihood test can be applied to test independence between 2 categorical variables
loglm(~1+2, con1)
#Next we discuss analysis for 3 way contingency table
#To create and save 3 way contingency table, xtabs function is used
con2 = xtabs(~jobclass+wage_cat+race, data=Wage)
ftable(con2)
#Conditional independence test for wage category against job class while controlling race using cochran-mantel-
#haenszel can be done with the following code
mantelhaen.test(con2)
#With this 3 way contingency table, we can also build the log linear model
freq2 = as.data.frame(as.table(con2)) #transform into frequency table
View(freq2)
View(freq2)
mod4 = glm(Freq~ jobclass + wage_cat + race, data = freq2, family = poisson)
summary(mod4)
mod4 = glm(Freq~ jobclass+wage_cat+race+jobclass:race, data = freq2, family = poisson)
summary(mod4)
mod4 = glm(Freq~ jobclass+wage_cat+race+jobclass:race+wage_cat:race, data = freq2, family = poisson)
summary(mod4)
mod4 = glm(Freq~ jobclass+wage_cat+race+jobclass:race+wage_cat:race+jobclass:wage_cat, data = freq2, family = poisson)
summary(mod4)
mod4 = glm(Freq~ jobclass+wage_cat+race+jobclass:race+wage_cat:race+jobclass:wage_cat+jobclass:wage_cat:race,
data = freq2, family = poisson)
summary(mod4)
library(readxl)
exe_1 <- read_excel("exe1_poisson-logistic_regression.xlsx")
View(exe_1)
library(readxl)
exe_2 <- read_excel("exe1_poisson-logistic_regression.xlsx",
sheet = "Sheet2")
View(exe_2)
hist(exe_2)
hist(exe_2$IPK)
exe1 = within(exe1, {
usia = factor(usia, levels=1:3, labels=c("<21", "21-50", ">50"))
jenis_kelamin = factor(jk, levels = 1:2, labels = c("Laki-laki", "Perempuan"))
ventilasi = factor(ventilasi, levels=1:3, labels=c("Kurang", "Sedang", "Cukup"))
})
exe_1 = within(exe_1, {
usia = factor(usia, levels=1:3, labels=c("<21", "21-50", ">50"))
jenis_kelamin = factor(jk, levels = 1:2, labels = c("Laki-laki", "Perempuan"))
ventilasi = factor(ventilasi, levels=1:3, labels=c("Kurang", "Sedang", "Cukup"))
})
View(exe_1)
summary(exe_1)
##Data
exe_1 = read_excel("exe1_poisson-logistic_regression.xlsx")
summary(exe_1)
require(ggplot2)
install.packages("ggplot2")
install.packages("sandwich")
install.packages("msm")
require(ggplot2)
require(sandwich)
require(msm)
##Data
exe_1 = read_excel("exe1_poisson-logistic_regression.xlsx")
exe_2 = read_excel("exe1_poisson-logistic_regression.xlsx",
sheet = "Sheet2")
exe_1 = within(exe_1, {
usia = factor(usia, levels=1:3, labels=c("<21", "21-50", ">50"))
jenis_kelamin = factor(jk, levels = 1:2, labels = c("Laki-laki", "Perempuan"))
ventilasi = factor(ventilasi, levels=1:3, labels=c("Kurang", "Sedang", "Cukup"))
})
summary(exe_1)
##Data
exe_1 = read_excel("exe1_poisson-logistic_regression.xlsx")
View(exe_1)
exe_1 = apply(exe_1, 2, as.factor)
summary(exe_1)
##Data
exe_1 = read_excel("exe1_poisson-logistic_regression.xlsx")
exe_2 = read_excel("exe1_poisson-logistic_regression.xlsx",
sheet = "Sheet2")
exe_1[,-2] = apply(exe_1[,-2], 2, as.factor)
summary(exe_1)
with(exe_1, tapply(count, usia, function(x) {
sprintf("M (SD) = %1.2f (%1.2f)", mean(x), sd(x))
}))
exe_2[,-1] = apply(exe_2[,-1], 2, as.factor)
summary(exe_2)
ggplot(exe_1, aes(count, fill = usia)) +
geom_histogram(binwidth=.5, position="dodge")
colnames(exe_1)[2]
colnames(exe_1)[2] = "count_tbc"
ggplot(exe_1, aes(count_tbc, fill = usia)) +
geom_histogram(binwidth=.5, position="dodge")
ggplot(exe_1, aes(count_tbc, fill = ventilasi)) +
geom_histogram(binwidth=.5, position="dodge")
poi_1 = glm(count_tbc ~ usia+jk+ventilasi, data = exe_1)
summary(poi_1)
poi_1 = glm(count_tbc ~ usia+jk+ventilasi, family = "poisson", data = exe_1)
summary(poi_1)
poi_1 = glm(count_tbc ~ ventilasi, family = "poisson", data = exe_1)
summary(poi_1)
poi_1 = glm(count_tbc ~ usia+ventilasi, family = "poisson", data = exe_1)
summary(poi_1)
poi_1 = glm(count_tbc ~ , family = "poisson", data = exe_1)
summary(poi_1)
poi_1 = glm(count_tbc ~. , family = "poisson", data = exe_1)
summary(poi_1)
poi_1 = glm(count_tbc ~rs , family = "poisson", data = exe_1)
summary(poi_1)
poi_1 = glm(count_tbc ~usia , family = "poisson", data = exe_1)
summary(poi_1)
poi_1 = glm(count_tbc ~ jk , family = "poisson", data = exe_1)
summary(poi_1)
poi_1 = glm(count_tbc ~ jk , family = "binomial", data = exe_1)
poi_1 = glm(count_tbc-1 ~ jk , family = "binomial", data = exe_1)
summary(poi_1)
poi_1 = glm(count_tbc-1 ~ ventilasi , family = "binomial", data = exe_1)
summary(poi_1)
poi_1 = glm(count_tbc-1 ~ usia+jk+ventilasi , family = "binomial", data = exe_1)
summary(poi_1)
poi_1 = glm(count_tbc-1 ~ usia+jk+ventilasi , family = "quasipoisson", data = exe_1)
summary(poi_1)
poi_1 = glm(count_tbc-1 ~ usia+jk , family = "quasipoisson", data = exe_1)
summary(poi_1)
class(exe_1$usia)
exe_1$usia = as.factor(exe_1$usia)
summary(exe_1)
exe_1$jk = as.factor(exe_1$jk)
exe_1$ventilasi = as.factor(exe_1$ventilasi)
summary(exe_1)
ggplot(exe_1, aes(count_tbc, fill = ventilasi)) +
geom_histogram(binwidth=.5, position="dodge")
poi_1 = glm(count_tbc ~ usia+jk , family = "poisson", data = exe_1)
summary(poi_1)
poi_1 = glm(count_tbc ~ usia+jk+ventilasi , family = "poisson", data = exe_1)
summary(poi_1)
poi_2 = glm(count_tbc ~ usia+ventilasi , family = "poisson", data = exe_1)
summary(poi_2)
exe_1$"usia2" = ifelse(exe_1$usia=="3_>50", "3_>50", "1_<=50")
poi_3 = glm(count_tbc ~ usia2+ventilasi , family = "poisson", data = exe_1)
summary(poi_3)
exe_1$"ventilasi2" = ifelse(exe_1$ventilasi=="2_s", "2_s", "1_ts")
poi_4 = glm(count_tbc ~ usia2+ventilasi2 , family = "poisson", data = exe_1)
summary(poi_4)
poi_5 = glm(count_tbc ~ ventilasi , family = "poisson", data = exe_1)
summary(poi_5)
poi_5 = glm(count_tbc ~ usia , family = "poisson", data = exe_1)
poi_5 = glm(count_tbc ~ usia , family = "poisson", data = exe_1)
summary(poi_5)
poi_5 = glm(count_tbc ~ jkp , family = "poisson", data = exe_1)
poi_5 = glm(count_tbc ~ jk , family = "poisson", data = exe_1)
summary(poi_5)
poi_5 = glm(count_tbc ~ ventilasi , family = "poisson", data = exe_1)
summary(poi_5)
#Since we are limited to only discuss the first order of GLM, we choose best model based on AIC.
#Thus a poisson regression with single predictor variable "ventilasi" is chosen (poi_5).
#Justification for using poisson regression is that the data has response variable
#of "count" type, that is for TBC disease patients.
#This particular data is not decently modeled by first order poisson regression.
#To intepret the coefficient, we use the "robust" version from Cameron and Trivedi (2009), which accounts for
#violation of assumption of mean = variance.
cov.poi5 = vcovHC(poi_5, type="HC0")
std.err = sqrt(diag(cov.poi5))
r.est = cbind(Estimate= coef(poi_5), "Robust SE" = std.err,
"Pr(>|z|)" = 2 * pnorm(abs(coef(poi_5)/std.err), lower.tail=FALSE),
LL = coef(poi_5) - 1.96 * std.err,
UL = coef(poi_5) + 1.96 * std.err)
r.est
#Since we are limited to only discuss the first order of GLM, we choose best model based on AIC.
#We can also do deviance statistics analysis to compare between models
with(poi_1, cbind(res.deviance = deviance, df = df.residual,
p = pchisq(deviance, df.residual, lower.tail=FALSE)))
#test model differences with chi square test
anova(poi_1, poi_2, test="Chisq")
anova(poi_1, poi_5, test="Chisq")
#test model differences with chi square test
anova(poi_2, poi_1, test="Chisq")
#test model differences with chi square test
anova(poi_2, poi_1, test="Chisq")
anova(poi_2, poi_5, test="Chisq")
anova(poi_5, poi_2, test="Chisq")
#test model differences with chi square test
anova(poi_2, poi_1, test="Chisq")
anova(poi_5, poi_2, test="Chisq")
anova(poi_5, poi_1, test="Chisq")
r.est
exe_2$Resiliensi = as.factor(exe_2$Resiliensi)
exe_2$`Suka Kuliah Online` = as.factor(exe_2$`Suka Kuliah Online`)
summary(exe_2)
install.packages("aod")
library(aod)
##Logistic Regression, for data 2
colnames(exe_2)[3]
##Logistic Regression, for data 2
colnames(exe_2)[3] = "sko"
logi1 = glm(sko ~ IPK + Resiliensi, family = "binomial", data = exe_2)
summary(logi1)
summary(exe_2)
exe_2 = read_excel("exe1_poisson-logistic_regression.xlsx",
sheet = "Sheet2")
exe_2$Resiliensi = as.factor(exe_2$Resiliensi)
exe_2$`Suka Kuliah Online` = as.factor(exe_2$`Suka Kuliah Online`)
summary(exe_2)
exe_2$IPK = as.numeric(exe_2$IPK)
summary(exe_2)
exe_2 = read_excel("exe1_poisson-logistic_regression.xlsx",
sheet = "Sheet2")
exe_2 = read_excel("exe1_poisson-logistic_regression.xlsx",
sheet = "Sheet2")
exe_2$Resiliensi = as.factor(exe_2$Resiliensi)
exe_2$`Suka Kuliah Online` = as.factor(exe_2$`Suka Kuliah Online`)
summary(exe_2)
logi1 = glm(sko ~ IPK + Resiliensi, family = "binomial", data = exe_2)
summary(logi1)
##Logistic Regression, for data 2
colnames(exe_2)[3] = "sko"
logi1 = glm(sko ~ IPK + Resiliensi, family = "binomial", data = exe_2)
summary(logi1)
#Warning message of numerical method error
#This is most likely due to outlier
logi1$fitted.values
#Warning message of numerical method error
#This is most likely due to outlier or maybe, it just has indistinguishable predicted and observed
#Lets inspect and compare predicted values vs observed values
exe_2$pred = predict(logi1, exe_2, type = "response")
View(exe_2)
summary(logi1)
install.packages("dplyr")
library(dplyr)
#In this case, we have whats also known as "perfect fit". Thus, we can simply ignore it.
#OR, we can try identify a quite obvious "separation rule" that is happening in the data
exe_2 %>% group_by(sko)
#In this case, we have whats also known as "perfect fit". Thus, we can simply ignore it.
#OR, we can try identify a quite obvious "separation rule" that is happening in the data
exe_2 %>% group_by(sko) %>% summarise(
min_ipk = min(IPK),
max_ipk = max(IPK))
install.packages("logistf")
library(logistf)
#Between "suka" and "tidak" the separation is perfectly dictated by variable "IPK"
#If, it is decided that "do nothing" is the choice, then simply interpret the coefficient with doing
#overall model check or significance test.
#For the other alternative, we can use penalized likelihood logisitc regression or also known as "firth logistic"
logi2 = logistf(sko ~ IPK + Resiliensi, data = exe_2)
summary(logi2)
#Interpreting using the firth logistic result above, it is found that "resiliensi" is not a useful predictor and
#we can simply build a simple logistic regression using "IPK" as predictor
logi3 = logistf(sko ~ IPK, data = exe_2)
summary(logi3)
ggplot(exe_1, aes(count_tbc, fill = ventilasi)) +
geom_histogram(binwidth=.5, position="dodge")
summary(poi_1)
summary(poi_2)
summary(poi_3)
summary(poi_4)
summary(poi_5)
summary(poi_5)
#test model differences with chi square test
anova(poi_2, poi_1, test="Chisq")
anova(poi_5, poi_2, test="Chisq")
anova(poi_5, poi_1, test="Chisq")
r.est
summary(logi1)
logi1 = glm(sko ~ IPK + Resiliensi, family = "binomial", data = exe_2)
summary(logi1)
summary(logi2)
